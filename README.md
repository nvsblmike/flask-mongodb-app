# Flask MongoDB Application Deployment on Minikube

This guide provides step-by-step instructions to deploy a Python Flask application connected to MongoDB on a Minikube Kubernetes cluster. The setup includes using Persistent Volumes (PV), Persistent Volume Claims (PVC), Kubernetes Secrets, Deployment and StatefulSets.

## Prerequisites

- **Minikube** installed and running.
- **Kubectl** installed and configured to interact with your Minikube cluster.
- **Docker** installed and running (for building and pushing Docker images).
- A Docker Hub account (or any other container registry account).

## 1. Download the project

```bash
git clone https://github.com/nvsblmike/flask-mongodb-app.git
cd flask-mongodb-app

## 2. Build and Push Docker Image

a. Build the Docker Image:
```bash
docker build -t <your-dockerhub-username>/flask-mongodb-app:latest .

b. Log in to Docker Hub:
```bash
docker login

c. Push the Docker Image:

```bash
docker push <your-dockerhub-username>/flask-mongodb-app:latest

## 3. Create Kubernetes Secrets

To create Kubernetes Secrets, run the command:

```bash
kubectl apply -f k8s/mongodb-secrets.yaml

## 4. Create Persistent Volumes and Claims

To create a persistent Volume and PVC with the PVC file, run:

```bash
kubectl apply -f k8s/mongodb-pvc.yaml

## 5. Deploy MongoDB StatefulSet

To create the StatefulSet and service, run:

``bash
kubectl apply -f k8s/mongodb-statefulset.yaml

You can check the deployment by running:

```bash
kubectl get pods -l app=mongodb

## 6. Deploy Flask Application

To create the Flask Deployment and service, run:

```bash
kubectl apply -f k8s/flask-deployment.yaml

## 8. Verify Your Deployment

a. You can firstly check your deployments and resources to see if they are properly configured by running:

```bash
kubectl get all

b. Then do a port forwarding to ensure the application is accessible on your browser:

```bash
kubectl port-forward svc/flask-service -p 5000:5000

c. Go ahead to check on your web browser

```bash
http://localhost:5000

d. You can test the endpoints by:

```bash
curl -X POST -H "Content-Type: application/json" -d '{"key":"value"}' http://localhost:5000

## 9. Clean Up Resources

To delete all resources follow these steps:
- firstly scale down the replicas

```bash
kubectl get deployments -o name | xargs -n 1 kubectl scale --replicas=0

- delete all resources

```bash
kubectl delete all --all

- delete other resources

```bash
kubectl delete configmaps --all
kubectl delete secrets --all
kubectl delete pvc --all

- running this should return no resources

```bash
kubectl get all


## DNS Resolution in Kubernetes

### How DNS Resolution Works in Kubernetes

In a Kubernetes cluster, DNS resolution plays an important role in ensuring seamless inter-pod communication happens. Kubernetes automatically creates DNS records for services and pods, allowing them to communicate with each other using simple DNS names rather than IP addresses. Here's how it works:

1. **Service Discovery**:
   - When a service is created in Kubernetes, a corresponding DNS entry is automatically generated. This DNS entry is in the format of `<service-name>.<namespace>.svc.cluster.local`.
   - Pods within the same namespace can resolve the service by simply using `<service-name>`. Pods in other namespaces can resolve it using `<service-name>.<namespace>`.

2. **Pod DNS Names**:
   - Each pod can be assigned a DNS name. The format is `<pod-ip>.<namespace>.pod.cluster.local`.
   - However, direct pod-to-pod communication using DNS is less common compared to service-to-pod communication.

3. **Cluster DNS Service**:
   - Kubernetes clusters typically include a DNS service that acts as a bridge between the cluster's DNS and external DNS services.
   - The DNS service continuously updates records to ensure that the IP addresses of services and pods are correctly resolved, enabling reliable communication even as pod IP addresses change due to scaling or pod restarts.

### Example of DNS Resolution in Flask-MongoDB Application

In the context of the Flask application connecting to MongoDB, DNS resolution allows the Flask application to refer to MongoDB by its service name (`mongodb-service`) rather than an IP address. This is how the Flask application can always connect to MongoDB, even if the MongoDB pod is rescheduled or its IP address changes.

The connection string in the Flask application would be:

```python
MONGODB_URI = "mongodb://<username>:<password>@mongodb-service:27017/"


## Resource Requests and Limits in Kubernetes

### Understanding Resource Requests and Limits

In Kubernetes, resource requests and limits are mechanisms used to manage the resources (CPU, memory) allocated to containers running in the cluster. Proper configuration of these parameters ensures efficient resource utilization, stability, and predictability of application performance.

1. **Resource Requests**:
   - **Definition**: A resource request specifies the minimum amount of CPU and memory that a container is guaranteed to receive. The Kubernetes scheduler uses the requests to determine which node can accommodate the container.
   - **Use Case**: Requests are essential for ensuring that critical containers have the necessary resources to function, even under heavy load. For example, if a container requests 0.2 CPU and 250 MiB of memory, the scheduler will ensure that at least 0.2 CPU and 250 MiB are available on the node before placing the container.

2. **Resource Limits**:
   - **Definition**: A resource limit defines the maximum amount of CPU and memory that a container is allowed to use. If the container tries to exceed this limit, Kubernetes will throttle CPU usage or kill the container if it exceeds memory limits.
   - **Use Case**: Limits prevent containers from consuming excessive resources, which could negatively impact other containers running on the same node. For example, setting a CPU limit of 0.5 and a memory limit of 500 MiB ensures that a container will not use more than these specified resources, protecting the overall stability of the node.

### Example Configuration of Resource Requests and Limits

In the deployment of the Flask application and MongoDB on Kubernetes, it was configured as follows:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask-app
.
.
.
        resources:
          requests:
            memory: "250Mi"
            cpu: "200m"
          limits:
            memory: "500Mi"
            cpu: "500m"
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongodb
spec:
  serviceName: "mongodb-service"
  replicas: 1
  template:
    spec:
      containers:
      - name: mongodb
        image: mongo:latest
        resources:
          requests:
            memory: "250Mi"
            cpu: "200m"
          limits:
            memory: "500Mi"
            cpu: "500m"


## Design Choices

### Kubernetes Configuration and Setup

#### Flask Application Deployment
- **Choice**: The Flask application was deployed using a Kubernetes Deployment with 2 replicas.
- **Reasoning**: Deployments are ideal for stateless applications like Flask. They provide easy scaling and ensure high availability through replica management. The choice of 2 replicas ensures redundancy and load balancing without overburdening the cluster resources.
- **Alternatives Considered**: 
  - **DaemonSet**: Not chosen because DaemonSets are more suitable for workloads that need to run on every node, which is not the case for this Flask application.
  - **Helm Chart**: Used to package and deploy the entire Flask application stack, providing an easy way to manage, upgrade, and version the deployment. The Helm chart abstracts the complexity of managing multiple YAML files.

#### MongoDB StatefulSet
- **Choice**: MongoDB was deployed using a StatefulSet.
- **Reasoning**: StatefulSets are designed for stateful applications like databases, where persistent storage, stable network identity, and ordered deployment are crucial. This ensures that MongoDB maintains its data across pod restarts and rescheduling.
- **Alternatives Considered**: 
  - **Deployment**: Not chosen because it does not guarantee stable storage or network identity, which is critical for database integrity.
  - **Helm Chart**: A MongoDB Helm chart was used for its pre-configured settings, allowing easy setup of authentication, storage, and service configurations. Helm simplifies the deployment process by providing reusable templates, making it easier to manage complex applications.

#### Persistent Volume (PV) and Persistent Volume Claim (PVC)
- **Choice**: Configured PV and PVC for MongoDB.
- **Reasoning**: PV and PVC were used to provide persistent storage to MongoDB, ensuring that data is retained even if the pod is deleted or rescheduled. This is crucial for maintaining data integrity in production environments.
- **Alternatives Considered**: 
  - **EmptyDir**: Not chosen because it is ephemeral storage, which does not persist data after pod deletion or rescheduling.
  - **HostPath**: Not chosen due to its limitations in a multi-node cluster and potential security risks.
  - **Helm Chart**: The Helm chart used for MongoDB includes built-in PV and PVC configurations, making it easier to set up and manage persistent storage.

#### Resource Requests and Limits
- **Choice**: Configured resource requests and limits for both Flask and MongoDB.
- **Reasoning**: Resource requests and limits were set to ensure that the pods have the necessary resources to operate efficiently and to prevent any single pod from consuming excessive resources. This setup helps maintain cluster stability and predictable application performance.
- **Alternatives Considered**:
  - **No Resource Limits**: Not chosen because it could lead to resource contention, where one pod monopolizes resources, potentially destabilizing the entire cluster.

### Helm Charts

- **Why Helm**: Helm was chosen as the package manager for Kubernetes due to its ability to manage complex Kubernetes applications as a single entity. It allows for easy upgrades, rollbacks, and version control of the entire stack, making it ideal for deploying both the Flask application and MongoDB.
- **Benefits**:
  - **Simplicity**: Helm charts simplify the deployment process by encapsulating all necessary Kubernetes resources into a single package.
  - **Reusability**: Helm charts can be reused across different environments (development, staging, production), ensuring consistency.
  - **Maintainability**: Helm provides an easy way to manage application upgrades and rollbacks, which is crucial for maintaining the application's lifecycle.

### DNS Resolution
- **Choice**: DNS was used for service discovery and inter-pod communication within the cluster.
- **Reasoning**: Kubernetes' built-in DNS service was chosen to ensure that the Flask application can reliably connect to MongoDB using a consistent service name, regardless of pod rescheduling.
- **Alternatives Considered**:
  - **Hardcoding IP Addresses**: Not chosen because it is inflexible and would break upon pod rescheduling.

### Autoscaling with HPA
- **Choice**: Horizontal Pod Autoscaler (HPA) was configured for the Flask application.
- **Reasoning**: HPA was chosen to ensure that the Flask application can scale up to handle increased load based on CPU utilization, improving application responsiveness and availability.
- **Alternatives Considered**:
  - **Manual Scaling**: Not chosen because it is not as responsive to changing workloads and requires manual intervention.

### Final Thoughts

The choices made in this deployment were driven by the need for reliability, scalability, and maintainability. Helm charts were integral in streamlining the deployment process, enabling consistent and repeatable deployments across environments. Each configuration was selected to align with best practices for Kubernetes deployments, ensuring a robust and scalable application setup.


## Testing Autoscaling and Database Interactions

### Autoscaling Testing

#### Steps to Test Autoscaling

1. **Initial Setup**:
   - I ensured that the Horizontal Pod Autoscaler (HPA) was properly configured for the Flask application.
   - I verified that the initial number of replicas was set to 2.

2. **Simulating High Traffic**:
   - I used `wrk` to generate high traffic to the Flask application.
   - Example using `wrk`:
     ```bash
     wrk -t12 -c400 -d30s http://localhost:5000/
     ```
   - This command simulated 12 threads, 400 connections, and ran for 30 seconds.

3. **Monitoring HPA**:
   - I checked the HPA status and metrics:

     ```bash
     kubectl get hpa
     kubectl describe hpa flask-app
     ```
   - I looked for changes in the number of replicas and CPU utilization metrics.

#### Results

- **Expected Outcome**: As traffic increases and CPU utilization exceeds 70%, the HPA should scale the number of Flask application replicas up to the maximum of 5.
- **Actual Outcome**: The HPA successfully scaled the number of replicas from 2 to 5 as CPU usage approached the defined threshold of 70%. The increased number of replicas handled the traffic spike without significant degradation in response time.

#### Issues Encountered

- **Issue 1**: There was a brief delay in scaling up the replicas due to a delay in detecting CPU utilization metrics.
  - **Resolution**: Increased the HPA's `--horizontal-pod-autoscaler-sync-period` to reduce the delay between metric collection and scaling actions.

### Database Interactions Testing

#### Steps to Test Database Interactions

1. **Initial Setup**:
   - I ensured that the Flask application and MongoDB were running with the appropriate configurations.
   - I verified that MongoDB authentication was enabled and that the Flask application used the correct credentials.

2. **Simulating Database Load**:
   - Use a script or tool to insert a large number of records into MongoDB to simulate a high load.
   - Example using a Python script:
     ```python
     import requests
     import json

     for i in range(1000):
         payload = json.dumps({"key": f"value{i}"})
         requests.post("http://localhost:5000/data", data=payload, headers={"Content-Type": "application/json"})
     ```

3. **Monitoring Database Performance**:
   - Use MongoDBâ€™s built-in monitoring tools or metrics to observe database performance.
   - Example:
     ```bash
     kubectl exec -it mongodb-pod -- mongo --eval "db.serverStatus()"
     ```
   - Check for latency, throughput, and any errors in the MongoDB logs.

#### Results

- **Expected Outcome**: MongoDB should handle the load without significant performance degradation or errors, and the Flask application should be able to insert and retrieve data as expected.
- **Actual Outcome**: MongoDB managed the high load effectively, with some increased latency observed during peak load times. The Flask application was able to insert and retrieve data correctly, but response times for `/data` endpoint increased under heavy load.

#### Issues Encountered

- **Issue 1**: Increased latency during high load periods.
  - **Resolution**: Increased the resources allocated to the MongoDB StatefulSet (CPU and memory) and optimized indexes to improve performance.

- **Issue 2**: Occasional database connection timeouts.
  - **Resolution**: Adjusted MongoDB connection settings and increased the connection pool size to accommodate higher traffic.

## Conclusion

This guide provided a complete step-by-step deployment of a Python Flask application connected to MongoDB on a Minikube Kubernetes cluster. The setup included building and pushing a Docker image, creating Kubernetes secrets, setting up persistent storage, deploying StatefulSets, and exposing services. The deployment was verified by accessing the Flask application and testing its endpoints.

This ReadMe provides a comprehensive guide, from setting up the environment to cleaning up resources.

It also includes a detailed explanation of how DNS resolution works in Kubernetes and how it applies to your Flask-MongoDB application deployment and provides a detailed explanation of resource requests and limits, along with an example configuration for your Flask application and MongoDB deployment in Kubernetes.

Testing confirmed that the autoscaling and database interactions are functioning as intended. The autoscaling setup efficiently managed increased traffic, and MongoDB handled the database load with minor adjustments. The testing helped identify areas for optimization, ensuring the system's stability and performance under varying conditions.
